{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document Retrieval",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71622f885c534351afacc8a53425b0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ffcd7f31879b4281a62534f3576fa171",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b4f097f02c364c4ab253c5f6728e4b3d",
              "IPY_MODEL_5c2bd00d2d1744fb82cae3faf6d90aaf"
            ]
          }
        },
        "ffcd7f31879b4281a62534f3576fa171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4f097f02c364c4ab253c5f6728e4b3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2ba453ddae1c4d79adb207607395ec76",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 18877,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 18877,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a246c546fd1f440aa1e8e9696714a212"
          }
        },
        "5c2bd00d2d1744fb82cae3faf6d90aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9fae9c7411cf4605ba5ca40bbe0fbeca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 18877/18877 [04:29&lt;00:00, 70.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41ce01a6a6104683ad3942d575df6463"
          }
        },
        "2ba453ddae1c4d79adb207607395ec76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a246c546fd1f440aa1e8e9696714a212": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9fae9c7411cf4605ba5ca40bbe0fbeca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41ce01a6a6104683ad3942d575df6463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwOfoX_srAhC",
        "outputId": "b2223a80-af62-4bfc-d0f1-ee4ff9e43555"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqSrExuFpqBk"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsdZ2dGvpclQ",
        "outputId": "2d61abed-8b38-42fd-dd02-72e58c18bf81"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()\n",
        "\n",
        "import string\n",
        "from heapq import nlargest\n",
        "import sys\n",
        "import spacy\n",
        "!pip install truecase\n",
        "import truecase\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "from nltk.tag import StanfordNERTagger\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from pprint import pprint\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
        "import warnings\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "from scipy.spatial import distance\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "!pip install sent2vec\n",
        "from sent2vec.vectorizer import Vectorizer as S2vectorizer\n",
        "tqdm_notebook.pandas()\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "!pip install pickle5\n",
        "import pickle5 as pickle  \n",
        "import networkx as nx\n",
        "\n",
        "#import pipelines\n",
        "from transformers import pipeline\n",
        "nlp2 = pipeline(\"question-answering\")\n",
        "\n",
        "pd.set_option('display.max_colwidth', 0)\n",
        "pd.set_option('display.max_columns', 0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: truecase in /usr/local/lib/python3.7/dist-packages (0.0.12)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from truecase) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->truecase) (1.15.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.4.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.95)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2020.12.5)\n",
            "Requirement already satisfied: sent2vec in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from sent2vec) (3.6.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from sent2vec) (2.2.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from sent2vec) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sent2vec) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from sent2vec) (1.8.1+cu101)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->sent2vec) (4.2.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (54.2.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->sent2vec) (2.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (3.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->sent2vec) (20.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->sent2vec) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->sent2vec) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->sent2vec) (3.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->sent2vec) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->sent2vec) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->sent2vec) (2.4.7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzXoa8Q7r5Eb"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5165fbRr4P8"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\n",
        "\n",
        "def clean_normalcase_stop_lem(text):\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\s{2,}', \" \", text)\n",
        "    text = unidecode(text)\n",
        "    text = ' '.join([wordnet_lemmatizer.lemmatize(i) for i in text.split()])\n",
        "    return text\n",
        "\n",
        "def clean_normalcase_nostop_lem(text):\n",
        "    text = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\s{2,}', \" \", text)\n",
        "    text = unidecode(text)\n",
        "    return text\n",
        "\n",
        "def clean_lowercase_stop_lem(text):\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\s{2,}', \" \", text)\n",
        "    text = unidecode(text)\n",
        "    text = ' '.join([wordnet_lemmatizer.lemmatize(i) for i in text.split()])\n",
        "    return text.lower()\n",
        "\n",
        "def clean_lowercase_nostop_lem(text):\n",
        "    text = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
        "#     text = ' '.join([ps.stem(word) for word in text.split() if word not in stop_words])\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\s{2,}', \" \", text)\n",
        "    text = unidecode(text)\n",
        "    return text.lower()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWaBI2d4nDN"
      },
      "source": [
        "def extractive_summariser(text):\n",
        "\n",
        "  # If the length of the text is greater than 20, take a 10th of the sentences\n",
        "\n",
        "  length = 3\n",
        "\n",
        "  # Remove punctuation\n",
        "  nopunc = [char for char in text if char not in string.punctuation]\n",
        "  nopunc = ''.join(nopunc)\n",
        "  # Remove stopwords\n",
        "  processed_text =[word for word in nopunc.split() if word.lower() not in nltk.corpus.stopwords.words('english')]\n",
        "\n",
        "  # Create a dictionary to store word frequency\n",
        "  word_freq = {}\n",
        "  # Enter each word and its number of occurrences\n",
        "  for word in processed_text:\n",
        "    if word not in word_freq:\n",
        "      word_freq[word] = 1\n",
        "    else:\n",
        "      word_freq[word] = word_freq[word] + 1\n",
        "\n",
        "  # Divide all frequencies by max frequency to give store of (0, 1]\n",
        "  max_freq = max(word_freq.values())\n",
        "  for word in word_freq.keys():\n",
        "    word_freq[word] = (word_freq[word]/max_freq)\n",
        "\n",
        "  # Create a list of the sentences in the text\n",
        "  sent_list = nltk.sent_tokenize(text)\n",
        "  # Create an empty dictionary to store sentence scores\n",
        "  sent_score = {}\n",
        "  for sent in sent_list:\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "      if word in word_freq.keys():\n",
        "        if sent not in sent_score.keys():\n",
        "          sent_score[sent] = word_freq[word]\n",
        "        else:\n",
        "          sent_score[sent] = sent_score[sent] + word_freq[word]\n",
        "\n",
        "  summary_sents = nlargest(length, sent_score, key = sent_score.get)\n",
        "  summary = ' '.join(summary_sents)\n",
        "  return summary"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLk6MzxJziK0"
      },
      "source": [
        "ldf['cleaned_normalcase_stop_lem'] = df['context'].progress_apply(clean_normalcase_stop_lem)\n",
        "df['cleaned_normalcase_nostop_lem'] = df['context'].progress_apply(clean_normalcase_nostop_lem)\n",
        "df['cleaned_lowercase_stop_lem'] = df['context'].progress_apply(clean_lowercase_stop_lem)\n",
        "df['cleaned_lowercase_nostop_lem'] = df['context'].progress_apply(clean_lowercase_nostop_lem)\n",
        "df['ner_combined'] = df['context_ner'].apply(lambda x: ' '.join(x).strip('[').strip(']').replace(',',''))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5HHI7E9zrbn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "71622f885c534351afacc8a53425b0ae",
            "ffcd7f31879b4281a62534f3576fa171",
            "b4f097f02c364c4ab253c5f6728e4b3d",
            "5c2bd00d2d1744fb82cae3faf6d90aaf",
            "2ba453ddae1c4d79adb207607395ec76",
            "a246c546fd1f440aa1e8e9696714a212",
            "9fae9c7411cf4605ba5ca40bbe0fbeca",
            "41ce01a6a6104683ad3942d575df6463"
          ]
        },
        "outputId": "5b237ec4-2020-4db9-c4e4-46d196fd71a4"
      },
      "source": [
        "df['extractive_summarized_3_sent'] = df['context'].progress_apply(extractive_summariser)\n",
        "df['extractive_summarized_3_sent'] = df['extractive_summarized_3_sent'].progress_apply(clean_lowercase_nostop_lem)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71622f885c534351afacc8a53425b0ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=18877.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD-EYxSz8O2k"
      },
      "source": [
        "df.to_pickle('/content/gdrive/MyDrive/tmproj/2_apr_brian.pkl')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9do35dYsIcD"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jZUX6KWsKUp"
      },
      "source": [
        "#Read the pickle file\n",
        "with open('/content/gdrive/MyDrive/tmproj/2_apr_brian.pkl', \"rb\") as fh:\n",
        "  df = pickle.load(fh)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6i0LZY8y8amt",
        "outputId": "ab03bf28-9be6-4493-900c-2d4b745d9e0c"
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>context_ner</th>\n",
              "      <th>cleaned_normalcase_stop_lem</th>\n",
              "      <th>cleaned_normalcase_nostop_lem</th>\n",
              "      <th>cleaned_lowercase_stop_lem</th>\n",
              "      <th>cleaned_lowercase_nostop_lem</th>\n",
              "      <th>ner_combined</th>\n",
              "      <th>extractive_summarized_3_sent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&amp;B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".</td>\n",
              "      <td>{the late 1990s, american, grammy awards, knowles-carter, 2003, mathew knowles, crazy in love, september 4, 1981, love, destiny's child, one, r&amp;b, beyonce, texas, baby boy, houston, 100}</td>\n",
              "      <td>Beyonce Giselle Knowles Carter bi:'janseI bee YON say born September 4 1981 is an American singer songwriter record producer and actress Born and raised in Houston Texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of R B girl group Destiny s Child Managed by her father Mathew Knowles the group became one of the world s best selling girl groups of all time Their hiatus saw the release of Beyonce s debut album Dangerously in Love 2003 which established her as a solo artist worldwide earned five Grammy Awards and featured the Billboard Hot 100 number one singles Crazy in Love and Baby Boy</td>\n",
              "      <td>Beyonce Giselle Knowles Carter bi:'janseI bee YON say born September 4 1981 American singer songwriter record producer actress Born raised Houston Texas performed various singing dancing competitions child rose fame late 1990s lead singer R B girl group Destiny s Child Managed father Mathew Knowles group became one world s best selling girl groups time Their hiatus saw release Beyonce s debut album Dangerously Love 2003 established solo artist worldwide earned five Grammy Awards featured Billboard Hot 100 number one singles Crazy Love Baby Boy</td>\n",
              "      <td>beyonce giselle knowles carter bi:'jansei bee yon say born september 4 1981 is an american singer songwriter record producer and actress born and raised in houston texas she performed in various singing and dancing competitions as a child and rose to fame in the late 1990s as lead singer of r b girl group destiny s child managed by her father mathew knowles the group became one of the world s best selling girl groups of all time their hiatus saw the release of beyonce s debut album dangerously in love 2003 which established her as a solo artist worldwide earned five grammy awards and featured the billboard hot 100 number one singles crazy in love and baby boy</td>\n",
              "      <td>beyonce giselle knowles carter bi:'jansei bee yon say born september 4 1981 american singer songwriter record producer actress born raised houston texas performed various singing dancing competition child rose fame late 1990s lead singer r b girl group destiny s child managed father mathew knowles group became one world s best selling girl group time their hiatus saw release beyonce s debut album dangerously love 2003 established solo artist worldwide earned five grammy awards featured billboard hot 100 number one single crazy love baby boy</td>\n",
              "      <td>american 2003 grammy awards texas september 4 1981 baby boy mathew knowles beyonce love 100 r&amp;b the late 1990s knowles-carter one houston destiny's child crazy in love</td>\n",
              "      <td>born raised houston texas performed various singing dancing competition child rose fame late 1990s lead singer r b girl group destiny s child their hiatus saw release beyonce s debut album dangerously love 2003 established solo artist worldwide earned five grammy awards featured billboard hot 100 number one single crazy love baby boy beyonce giselle knowles carter bi:'jansei bee yon say born september 4 1981 american singer songwriter record producer actress</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.</td>\n",
              "      <td>{1970s, 2006, 2013, 1990s, sasha fierce, third, fourth, fifth, beyonce, 1980s, 2010, 2009, 2011, b'day, 4, six, irreplaceable, destiny's child, dreamgirls, june 2005, the pink panther, jay z, second, song of the year, etta james}</td>\n",
              "      <td>Following the disbandment of Destiny s Child in June 2005 she released her second solo album B Day 2006 which contained hits Deja Vu Irreplaceable and Beautiful Liar Beyonce also ventured into acting with a Golden Globe nominated performance in Dreamgirls 2006 and starring roles in The Pink Panther 2006 and Obsessed 2009 Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records 2008 influenced her third album I Am Sasha Fierce 2008 which saw the birth of her alter ego Sasha Fierce and earned a record setting six Grammy Awards in 2010 including Song of the Year for Single Ladies Put a Ring on It Beyonce took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul Her critically acclaimed fifth studio album Beyonce 2013 was distinguished from previous releases by its experimental production and exploration of darker themes</td>\n",
              "      <td>Following disbandment Destiny s Child June 2005 released second solo album B Day 2006 contained hits Deja Vu Irreplaceable Beautiful Liar Beyonce also ventured acting Golden Globe nominated performance Dreamgirls 2006 starring roles The Pink Panther 2006 Obsessed 2009 Her marriage rapper Jay Z portrayal Etta James Cadillac Records 2008 influenced third album I Am Sasha Fierce 2008 saw birth alter ego Sasha Fierce earned record setting six Grammy Awards 2010 including Song Year Single Ladies Put Ring It Beyonce took hiatus music 2010 took management career fourth album 4 2011 subsequently mellower tone exploring 1970s funk 1980s pop 1990s soul Her critically acclaimed fifth studio album Beyonce 2013 distinguished previous releases experimental production exploration darker themes</td>\n",
              "      <td>following the disbandment of destiny s child in june 2005 she released her second solo album b day 2006 which contained hits deja vu irreplaceable and beautiful liar beyonce also ventured into acting with a golden globe nominated performance in dreamgirls 2006 and starring roles in the pink panther 2006 and obsessed 2009 her marriage to rapper jay z and portrayal of etta james in cadillac records 2008 influenced her third album i am sasha fierce 2008 which saw the birth of her alter ego sasha fierce and earned a record setting six grammy awards in 2010 including song of the year for single ladies put a ring on it beyonce took a hiatus from music in 2010 and took over management of her career her fourth album 4 2011 was subsequently mellower in tone exploring 1970s funk 1980s pop and 1990s soul her critically acclaimed fifth studio album beyonce 2013 was distinguished from previous releases by its experimental production and exploration of darker themes</td>\n",
              "      <td>following disbandment destiny s child june 2005 released second solo album b day 2006 contained hit deja vu irreplaceable beautiful liar beyonce also ventured acting golden globe nominated performance dreamgirls 2006 starring role the pink panther 2006 obsessed 2009 her marriage rapper jay z portrayal etta james cadillac records 2008 influenced third album i am sasha fierce 2008 saw birth alter ego sasha fierce earned record setting six grammy awards 2010 including song year single ladies put ring it beyonce took hiatus music 2010 took management career fourth album 4 2011 subsequently mellower tone exploring 1970s funk 1980s pop 1990s soul her critically acclaimed fifth studio album beyonce 2013 distinguished previous release experimental production exploration darker themes</td>\n",
              "      <td>etta james the pink panther fourth six beyonce third 2009 b'day 2013 dreamgirls 1980s 1990s 2010 sasha fierce 4 song of the year 1970s destiny's child 2011 fifth second june 2005 jay z irreplaceable 2006</td>\n",
              "      <td>beyonce took hiatus music 2010 took management career fourth album 4 2011 subsequently mellower tone exploring 1970s funk 1980s pop 1990s soul her marriage rapper jay z portrayal etta james cadillac records 2008 influenced third album i am sasha fierce 2008 saw birth alter ego sasha fierce earned record setting six grammy awards 2010 including song year single ladies put ring it her critically acclaimed fifth studio album beyonce 2013 distinguished previous release experimental production exploration darker themes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.</td>\n",
              "      <td>{the top female artist, the 2000s decade, 2013, 19 years, the top certified artist in america, artist of the millennium, beyonce, 100, 118 million, 20 grammy awards, 60 million, 2015, 2009, the recording industry association of america, 2011, the top radio songs artist of, billboard, 2014, destiny's child, forbes, time}</td>\n",
              "      <td>A self described modern day feminist Beyonce creates songs that are often characterized by themes of love relationships and monogamy as well as female sexuality and empowerment On stage her dynamic highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music Throughout a career spanning 19 years she has sold over 118 million records as a solo artist and a further 60 million with Destiny s Child making her one of the best selling music artists of all time She has won 20 Grammy Awards and is the most nominated woman in the award s history The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade In 2009 Billboard named her the Top Radio Songs Artist of the Decade the Top Female Artist of the 2000s and their Artist of the Millennium in 2011 Time listed her among the 100 most influential people in the world in 2013 and 2014 Forbes magazine also listed her as the most powerful female musician of 2015</td>\n",
              "      <td>A self described modern day feminist Beyonce creates songs often characterized themes love relationships monogamy well female sexuality empowerment On stage dynamic highly choreographed performances led critics hailing one best entertainers contemporary popular music Throughout career spanning 19 years sold 118 million records solo artist 60 million Destiny s Child making one best selling music artists time She 20 Grammy Awards nominated woman award s history The Recording Industry Association America recognized Top Certified Artist America 2000s decade In 2009 Billboard named Top Radio Songs Artist Decade Top Female Artist 2000s Artist Millennium 2011 Time listed among 100 influential people world 2013 2014 Forbes magazine also listed powerful female musician 2015</td>\n",
              "      <td>a self described modern day feminist beyonce creates songs that are often characterized by themes of love relationships and monogamy as well as female sexuality and empowerment on stage her dynamic highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music throughout a career spanning 19 years she has sold over 118 million records as a solo artist and a further 60 million with destiny s child making her one of the best selling music artists of all time she has won 20 grammy awards and is the most nominated woman in the award s history the recording industry association of america recognized her as the top certified artist in america during the 2000s decade in 2009 billboard named her the top radio songs artist of the decade the top female artist of the 2000s and their artist of the millennium in 2011 time listed her among the 100 most influential people in the world in 2013 and 2014 forbes magazine also listed her as the most powerful female musician of 2015</td>\n",
              "      <td>a self described modern day feminist beyonce creates song often characterized theme love relationships monogamy well female sexuality empowerment on stage dynamic highly choreographed performance led critic hailing one best entertainer contemporary popular music throughout career spanning 19 years sold 118 million record solo artist 60 million destiny s child making one best selling music artist time she 20 grammy awards nominated woman award s history the recording industry association america recognized top certified artist america 2000s decade in 2009 billboard named top radio songs artist decade top female artist 2000s artist millennium 2011 time listed among 100 influential people world 2013 2014 forbes magazine also listed powerful female musician 2015</td>\n",
              "      <td>artist of the millennium billboard beyonce 2009 the recording industry association of america 2013 2015 the top certified artist in america 2014 118 million the top radio songs artist of 60 million forbes 100 20 grammy awards 2011 destiny's child the 2000s decade the top female artist 19 years time</td>\n",
              "      <td>throughout career spanning 19 years sold 118 million record solo artist 60 million destiny s child making one best selling music artist time on stage dynamic highly choreographed performance led critic hailing one best entertainer contemporary popular music a self described modern day feminist beyonce creates song often characterized theme love relationships monogamy well female sexuality empowerment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              extractive_summarized_3_sent\n",
              "0   Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".                                                                                                                                                                                                                                                                                                                                                                                  ...  born raised houston texas performed various singing dancing competition child rose fame late 1990s lead singer r b girl group destiny s child their hiatus saw release beyonce s debut album dangerously love 2003 established solo artist worldwide earned five grammy awards featured billboard hot 100 number one single crazy love baby boy beyonce giselle knowles carter bi:'jansei bee yon say born september 4 1981 american singer songwriter record producer actress                                                          \n",
              "15  Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits \"Déjà Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyoncé also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyoncé took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyoncé (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.                                                ...  beyonce took hiatus music 2010 took management career fourth album 4 2011 subsequently mellower tone exploring 1970s funk 1980s pop 1990s soul her marriage rapper jay z portrayal etta james cadillac records 2008 influenced third album i am sasha fierce 2008 saw birth alter ego sasha fierce earned record setting six grammy awards 2010 including song year single ladies put ring it her critically acclaimed fifth studio album beyonce 2013 distinguished previous release experimental production exploration darker themes \n",
              "27  A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.  ...  throughout career spanning 19 years sold 118 million record solo artist 60 million destiny s child making one best selling music artist time on stage dynamic highly choreographed performance led critic hailing one best entertainer contemporary popular music a self described modern day feminist beyonce creates song often characterized theme love relationships monogamy well female sexuality empowerment                                                                                                                     \n",
              "\n",
              "[3 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ5lkkvRq3PP"
      },
      "source": [
        "## NER Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drU_S8tppcwS",
        "outputId": "a61d2263-1b34-4b77-b76a-c8d72f442f18"
      },
      "source": [
        "pprint([(X.text, X.label_) for X in nlp(\"America is a country\").ents])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('America', 'GPE')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONIOaCZHpcxo"
      },
      "source": [
        "def generate_ners(text):\n",
        "    result = set()\n",
        "    for X in nlp(text).ents:\n",
        "        result.add(unidecode(X.text.lower()))\n",
        "    return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyQO0o4Upcy3"
      },
      "source": [
        "df = pd.read_csv('../data/SQuAD_csv.csv', encoding='utf-8').loc[:, ['context']]\n",
        "df = df.drop_duplicates(subset=['context'])\n",
        "df['context_ner'] = df['context'].progress_apply(generate_ners)\n",
        "# df['context'] = df['context'].apply(lambda x: str(x).lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0MHgYFYsxad"
      },
      "source": [
        "## Method 1: NER TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pieeptGapc0N"
      },
      "source": [
        "# TF-IDF Vectorizer for ner_combined column\n",
        "ner_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "ner_tfidf = ner_vectorizer.fit_transform(df['ner_combined'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VyEA17yXpc1j",
        "outputId": "f7928510-8457-416a-e9f0-1f362d40d3c2"
      },
      "source": [
        "# Query preprocessing for NER document retrieval\n",
        "def query_to_ner_str(query):\n",
        "    result = re.sub('[%s]' % re.escape(string.punctuation), '', query)\n",
        "    result = re.sub('\\s{2,}', \" \", result)\n",
        "    result = ' '.join([word for word in result.split() if word.lower() not in stop_words])\n",
        "    result = ' '.join([X.text.lower() for X in nlp(truecase.get_true_case(result)).ents])\n",
        "\n",
        "    if result == '':\n",
        "        return None\n",
        "    return result\n",
        "\n",
        "# Testing query_to_ner_str function\n",
        "query_to_ner_str('what is the political system of the islamic republic based on')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'islamic republic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4laTKndpc24"
      },
      "source": [
        "def document_retrieval_ner(df, ner_vectorizer, ner_tfidf, query):\n",
        "    ner_str = query_to_ner_str(query)\n",
        "\n",
        "    if ner_str is None:\n",
        "        return None\n",
        "    \n",
        "    print(f\"NER String: {ner_str}\\n\")\n",
        "    \n",
        "    ner_matching_ids = get_similar_docs(df, ner_vectorizer, ner_tfidf, ner_str)\n",
        "    \n",
        "    return ner_matching_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPBae4q5pc4Z"
      },
      "source": [
        "def get_similar_docs(df, tfidfvectorizer, docs_tfidf, query):\n",
        "    \"\"\"\n",
        "    vectorizer: TfIdfVectorizer model\n",
        "    docs_tfidf: tfidf vectors for all docs\n",
        "    query: query\n",
        "\n",
        "    return: doc with highest tf-idf cosine similarity\n",
        "    \"\"\"\n",
        "    query_tfidf = tfidfvectorizer.transform([query])\n",
        "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
        "    max_sim = max(cosineSimilarities)\n",
        "    \n",
        "    if max_sim < 0.05: # not sure whether to set this threshold as some correct answers are like 0.1 similarity\n",
        "        print(\"No Matches\")\n",
        "        return None\n",
        "    else:\n",
        "        threshold = 0.6 * max_sim\n",
        "    \n",
        "    top_doc_ids = set()\n",
        "    for idx, val in enumerate(cosineSimilarities):\n",
        "        if val >= threshold:\n",
        "            top_doc_ids.add((idx,val))\n",
        "            \n",
        "    top_doc_ids = sorted(top_doc_ids, key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "    print(f\"Top Docs: {top_doc_ids}\\n\")\n",
        "            \n",
        "    return top_doc_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZEuMM2Npc5x",
        "outputId": "68a968a0-eb56-4fb3-c4ff-e36eb446aaf3"
      },
      "source": [
        "query = 'what does polytechnic mean in singapore?'\n",
        "retrieved_doc_ids = document_retrieval_ner(df, ner_vectorizer, ner_tfidf, query)\n",
        "print(df.iloc[[i[0] for i in retrieved_doc_ids], df.columns.get_loc('context')].apply(lambda x: [i.strip() for i in x.split('.') if len(i)>1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NER String: singapore\n",
            "\n",
            "Top Docs: [(1383, 0.3412023014328927), (1382, 0.23353586294956077), (8912, 0.20957190592726097)]\n",
            "\n",
            "7599     [Polytechnics offer three-year diploma courses in fields such as information technology, engineering subjects and other vocational fields, like psychology and nursing, There are 5 polytechnics in Singapore, They are namely:]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "7598     [Polytechnics in Singapore provides industry oriented education equivalent to a junior college or sixth form college in the UK, Singapore retains a system similar but not the same as in the United Kingdom from 1970–1992, distinguishing between polytechnics and universities, Unlike the British Polytechnic (United Kingdom) system Singapore Polytechnics do not offer bachelors, masters or PhD degrees, Under this system, most Singaporean students sit for their O-Level examinations after a four or five years of education in secondary school, and apply for a place at either a technical school termed ITE, a polytechnic or a university-preparatory school (a junior college or the Millennia Institute, a centralized institute), Polytechnic graduates may be granted transfer credits when they apply to local and overseas universities, depending on the overall performance in their grades, as well as the university's policies on transfer credits, A few secondary schools are now offering six-year program which leads directly to university entrance]\n",
            "40713    [The region's economy greatly depends on agriculture; rice and rubber have long been prominent exports, Manufacturing and services are becoming more important, An emerging market, Indonesia is the largest economy in this region, Newly industrialised countries include Indonesia, Malaysia, Thailand, and the Philippines, while Singapore and Brunei are affluent developed economies, The rest of Southeast Asia is still heavily dependent on agriculture, but Vietnam is notably making steady progress in developing its industrial sectors, The region notably manufactures textiles, electronic high-tech goods such as microprocessors and heavy industrial products such as automobiles, Oil reserves in Southeast Asia are plentiful]                                                                                                                                                                                                                                                                                                                                  \n",
            "Name: context, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzL7wscAs3Fb"
      },
      "source": [
        "## Method 2: Paragraph TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4wBOZ3bpc8q"
      },
      "source": [
        "# TF-IDF Vectorizer for cleaned_lowercase_nostop_lem column\n",
        "para_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "para_tfidf = para_vectorizer.fit_transform(df['cleaned_lowercase_nostop_lem'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CJ5idSWepc-F",
        "outputId": "974c011d-07e8-48ea-de88-5ccfe532126a"
      },
      "source": [
        "# Query preprocessing for paragraph document retrieval\n",
        "def query_normal_cleaning(query):\n",
        "    result = re.sub('[%s]' % re.escape(string.punctuation), '', query)\n",
        "    result = re.sub('\\s{2,}', \" \", result).lower()\n",
        "    result = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in result.split() if word not in stop_words])\n",
        "\n",
        "    if result == '':\n",
        "        return None\n",
        "    return result\n",
        "\n",
        "# Testing query_to_ner_str function\n",
        "query_normal_cleaning('what is the political system of the islamic republic based on')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'political system islamic republic based'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zh-l-AKpc_j"
      },
      "source": [
        "def document_retrieval_para(df, para_vectorizer, para_tfidf, query):\n",
        "    cleaned = query_normal_cleaning(query)\n",
        "\n",
        "    if cleaned is None:\n",
        "        return None\n",
        "    \n",
        "    print(f\"Cleaned Query: {cleaned}\\n\")\n",
        "    \n",
        "    para_matching_ids = get_similar_docs(df, para_vectorizer, para_tfidf, cleaned)\n",
        "    \n",
        "    return para_matching_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXpFv8CqpdBj",
        "outputId": "fc5cfb20-6b3d-40fd-a977-4a1b8fc1d62d"
      },
      "source": [
        "query = 'what does polytechnic mean in singapore?'\n",
        "retrieved_doc_ids = document_retrieval_para(df, para_vectorizer, para_tfidf, query)\n",
        "if retrieved_doc_ids:\n",
        "    print(df.iloc[[i[0] for i in retrieved_doc_ids], df.columns.get_loc('context')].apply(lambda x: [i.strip() for i in x.split('.') if len(i)>1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cleaned Query: polytechnic mean singapore\n",
            "\n",
            "Top Docs: [(1341, 0.2742364173820103), (1382, 0.25939776890594335), (1360, 0.2366924919912639), (1375, 0.23299376347972817), (1391, 0.2048776630550047), (8918, 0.1885104228610306), (1383, 0.17557414483448747)]\n",
            "\n",
            "7520     [In Croatia there are many polytechnic institutes and colleges that offer a polytechnic education, The law about polytechnic education in Croatia was passed in 1997]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "7598     [Polytechnics in Singapore provides industry oriented education equivalent to a junior college or sixth form college in the UK, Singapore retains a system similar but not the same as in the United Kingdom from 1970–1992, distinguishing between polytechnics and universities, Unlike the British Polytechnic (United Kingdom) system Singapore Polytechnics do not offer bachelors, masters or PhD degrees, Under this system, most Singaporean students sit for their O-Level examinations after a four or five years of education in secondary school, and apply for a place at either a technical school termed ITE, a polytechnic or a university-preparatory school (a junior college or the Millennia Institute, a centralized institute), Polytechnic graduates may be granted transfer credits when they apply to local and overseas universities, depending on the overall performance in their grades, as well as the university's policies on transfer credits, A few secondary schools are now offering six-year program which leads directly to university entrance]                                                                                                                                                                                   \n",
            "7553     [The first polytechnic in Hong Kong is The Hong Kong Polytechnic, established in 1972 through upgrading the Hong Kong Technical College (Government Trade School before 1947), The second polytechnic, the City Polytechnic of Hong Kong, was founded in 1984, These polytechnics awards diplomas, higher diplomas, as well as academic degrees, Like the United Kingdom, the two polytechnics were granted university status in 1994, and renamed The Hong Kong Polytechnic University and the City University of Hong Kong respectively, The Hong Kong University of Science and Technology, a university with a focus in applied science, engineering and business, was founded in 1991]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "7585     [Since the 1990s, there has been consolidation in New Zealand's state-owned tertiary education system, In the polytechnic sector: Wellington Polytechnic amalgamated with Massey University, The Central Institute of Technology explored a merger with the Waikato Institute of Technology, which was abandoned, but later, after financial concerns, controversially amalgamated with Hutt Valley Polytechnic, which in turn became Wellington Institute of Technology, Some smaller polytechnics in the North Island, such as Waiarapa Polytechnic, amalgamated with UCOL, (The only other amalgamations have been in the colleges of education]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "7621     [Polytechnics were tertiary education teaching institutions in England, Wales and Northern Ireland, Since 1970 UK Polytechnics operated under the binary system of education along with universities, Polytechnics offered diplomas and degrees (bachelor's, master's, PhD) validated at the national level by the UK Council for National Academic Awards CNAA, They particularly excelled in engineering and applied science degree courses similar to technological universities in the USA and continental Europe, The comparable institutions in Scotland were collectively referred to as Central Institutions, Britain's first Polytechnic, the Royal Polytechnic Institution later known as the Polytechnic of Central London (now the University of Westminster) was established in 1838 at Regent Street in London and its goal was to educate and popularize engineering and scientific knowledge and inventions in Victorian Britain \"at little expense, \" The London Polytechnic led a mass movement to create numerous Polytechnic institutes across the UK in the late 19th Century, Most Polytechnic institutes were established at the centre of major metropolitan cities and their focus was on engineering, applied science and technology education]\n",
            "40738    [The culture in Southeast Asia is very diverse: on mainland Southeast Asia, the culture is a mix of Indochinese (Burma, Cambodia, Laos and Thailand) and Chinese (Singapore and Vietnam), While in Indonesia, the Philippines and Malaysia the culture is a mix of indigenous Austronesian, Indian, Islamic, Western, and Chinese cultures, Also Brunei shows a strong influence from Arabia, Singapore and Vietnam show more Chinese influence in that Singapore, although being geographically a Southeast Asian nation, is home to a large Chinese majority and Vietnam was in China's sphere of influence for much of its history, Indian influence in Singapore is only evident through the Tamil migrants, which influenced, to some extent, the cuisine of Singapore, Throughout Vietnam's history, it has had no direct influence from India - only through contact with the Thai, Khmer and Cham peoples]                                                                                                                                                                                                                                                                                                                                                       \n",
            "7599     [Polytechnics offer three-year diploma courses in fields such as information technology, engineering subjects and other vocational fields, like psychology and nursing, There are 5 polytechnics in Singapore, They are namely:]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
            "Name: context, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQTn6mCUtFVJ"
      },
      "source": [
        "## Method 3: NER + Paragraph TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLU-vinopdC_"
      },
      "source": [
        "def document_retrieval_ner_para(df, ner_vectorizer, ner_tfidf, para_vectorizer, para_tfidf, query):\n",
        "    df = df.reset_index()\n",
        "    ner_str = query_to_ner_str(query)\n",
        "    ner_docs = []\n",
        "    \n",
        "    cleaned = query_normal_cleaning(query)\n",
        "\n",
        "    if cleaned is None:\n",
        "        return None\n",
        "\n",
        "    if ner_str:    \n",
        "        print('========== EXECUTING NER TF-IDF ==========')\n",
        "        print(f\"NER String: {ner_str}\\n\")\n",
        "\n",
        "        ner_matching_ids = get_similar_docs(df, ner_vectorizer, ner_tfidf, ner_str)\n",
        "\n",
        "        if ner_matching_ids:\n",
        "          narrowed_paras_df = df.iloc[[i[0] for i in ner_matching_ids],:]\n",
        "          para_index = narrowed_paras_df.index\n",
        "                \n",
        "          # Re-run para_vectorizer and para_tfidf on narrowed down documents\n",
        "          para_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "          para_tfidf = para_vectorizer.fit_transform(narrowed_paras_df['cleaned_lowercase_nostop_lem'])\n",
        "        else:\n",
        "          para_index = df.index\n",
        "    else:\n",
        "        para_index = df.index\n",
        "        \n",
        "    print('=== EXECUTING PARA TF-IDF ===')\n",
        "    print(f\"Query String: {cleaned}\\n\")\n",
        "    query_tfidf = para_vectorizer.transform([cleaned])\n",
        "    cosineSimilarities = cosine_similarity(query_tfidf, para_tfidf).flatten()\n",
        "    max_sim = max(cosineSimilarities)\n",
        "    \n",
        "    if max_sim < 0.05: # not sure whether to set this threshold as some correct answers are like 0.1 similarity\n",
        "        print(\"No Matches\")\n",
        "        return None\n",
        "    else:\n",
        "        threshold = 0.4 * max_sim\n",
        "    \n",
        "    top_doc_ids = set()\n",
        "    for idx, val in enumerate(cosineSimilarities):\n",
        "        if val >= threshold:\n",
        "            top_doc_ids.add((para_index[idx],val))\n",
        "            \n",
        "    top_doc_ids = sorted(top_doc_ids, key=lambda x: x[1], reverse=True)\n",
        "            \n",
        "    print(f\"Top Docs: {top_doc_ids}\\n\")\n",
        "            \n",
        "    return top_doc_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C21r7Q-tFqx",
        "outputId": "0ae377b5-c30c-4108-8c79-89124c9ba4fe"
      },
      "source": [
        "query = input()\n",
        "print('------------------------------------------------------------------------------')\n",
        "retrieved_doc_ids = document_retrieval_ner_para(df, ner_vectorizer, ner_tfidf, para_vectorizer, para_tfidf, query)\n",
        "if retrieved_doc_ids:\n",
        "    print(df.iloc[[i[0] for i in retrieved_doc_ids], df.columns.get_loc('context')].apply(lambda x: [i.strip() for i in x.split('.') if len(i)>1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what does polytechnic mean in singapore\n",
            "------------------------------------------------------------------------------\n",
            "========== EXECUTING NER TF-IDF ==========\n",
            "NER String: singapore\n",
            "\n",
            "Top Docs: [(1383, 0.3412023014328927), (1382, 0.23353586294956077), (8912, 0.20957190592726097)]\n",
            "\n",
            "=== EXECUTING PARA TF-IDF ===\n",
            "Query String: polytechnic mean singapore\n",
            "\n",
            "Top Docs: [(1382, 0.22061356115579817), (1383, 0.155714613958824)]\n",
            "\n",
            "7598    [Polytechnics in Singapore provides industry oriented education equivalent to a junior college or sixth form college in the UK, Singapore retains a system similar but not the same as in the United Kingdom from 1970–1992, distinguishing between polytechnics and universities, Unlike the British Polytechnic (United Kingdom) system Singapore Polytechnics do not offer bachelors, masters or PhD degrees, Under this system, most Singaporean students sit for their O-Level examinations after a four or five years of education in secondary school, and apply for a place at either a technical school termed ITE, a polytechnic or a university-preparatory school (a junior college or the Millennia Institute, a centralized institute), Polytechnic graduates may be granted transfer credits when they apply to local and overseas universities, depending on the overall performance in their grades, as well as the university's policies on transfer credits, A few secondary schools are now offering six-year program which leads directly to university entrance]\n",
            "7599    [Polytechnics offer three-year diploma courses in fields such as information technology, engineering subjects and other vocational fields, like psychology and nursing, There are 5 polytechnics in Singapore, They are namely:]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "Name: context, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHx3VhrNtrsZ"
      },
      "source": [
        "## Method 4: Latent Semantic Indexing (LSI)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeAzClwUtHd6"
      },
      "source": [
        "from numpy.linalg import svd, norm\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "from collections import defaultdict, Counter"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yu52QommtsnR"
      },
      "source": [
        "class LSI:\n",
        "    def __repr__(self):\n",
        "        return 'LSI( terms:{}, documents:{}, index_ready:{})'.format(self.index.__len__(),\n",
        "                                                                     self.documents.__len__(),\n",
        "                                                                     not(self.__update_index))\n",
        "    \n",
        "    def __init__(self, tokenizer=nltk.word_tokenize,\n",
        "                 stemmer=EnglishStemmer,\n",
        "                 stopwords=nltk.corpus.stopwords.words('english'),\n",
        "                 variance=0.9):\n",
        "        '''\n",
        "        >>> queries cannot work unless __update_index is false.        \n",
        "        '''\n",
        "        self.stemmer = stemmer()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stopwords = stopwords\n",
        "        \n",
        "        self.documents = {}\n",
        "        self.index = defaultdict(Counter)\n",
        "        \n",
        "        self.A = None # term document matrix\n",
        "        self.U = None # output of svd\n",
        "        self.S = None # output of svd\n",
        "        self.V = None # output of svd\n",
        "        \n",
        "        self.term_rep = np.array(None) # reduced representation of terms after svd\n",
        "        self.doc_rep = np.array(None)  # reduced representation of documents after svd\n",
        "        \n",
        "        self.__update_index = True\n",
        "        self._term_index_in_A = {}\n",
        "        self._document_index_in_A = {}\n",
        "        \n",
        "        self.k = None # reduced dimension after SVD\n",
        "        self.variance = variance # variance to retain after SVD\n",
        "            \n",
        "        \n",
        "    def add_doc(self, document, document_id):\n",
        "        '''\n",
        "        add terms into vocabulary.\n",
        "        add document \n",
        "        '''\n",
        "        if document_id in self.documents:\n",
        "            print('document_id : {} already indexed.'.format(document_id))\n",
        "            return False\n",
        "        \n",
        "        for token in [t.lower() for t in self.tokenizer(document) if t.isalpha()]:\n",
        "            if token in self.stopwords:\n",
        "                continue;\n",
        "            if self.stemmer:\n",
        "                token = self.stemmer.stem(token)\n",
        "                \n",
        "            # add this token to defaultdict(Counter)\n",
        "            # this document's count is increased by 1 for this token's Counter\n",
        "            self.index[token].update({document_id:1})\n",
        "        \n",
        "        self.__update_index = True # update flag to rebuild index\n",
        "        self.documents[document_id] = document # add document to documents\n",
        "        return True\n",
        "    \n",
        "    \n",
        "    def _svd_A(self):\n",
        "        '''\n",
        "        Perform SVD on A and update the U,S,V matrices\n",
        "        '''\n",
        "        self.U, self.S, self.V = svd(self.A)\n",
        "        \n",
        "    \n",
        "    def _get_k_for_svd(self):\n",
        "        '''\n",
        "        Finds the value for k after SVD such that specified variance is retained\n",
        "        returns k : int\n",
        "        '''\n",
        "        if (self.S is not None):\n",
        "            sum = 0\n",
        "            k = 0\n",
        "            while(sum < self.variance):\n",
        "                k -=- 1\n",
        "                sum = self.S[:k].sum() / self.S.sum()\n",
        "            self.k = k\n",
        "            return True\n",
        "        else:\n",
        "            print('S is not populated.')\n",
        "            return False\n",
        "\n",
        "    def rebuild_index(self):\n",
        "        '''\n",
        "        >>> set _update_index to false when index is built\n",
        "        '''\n",
        "        terms = list(self.index.keys())\n",
        "        documents = list(self.documents.keys())\n",
        "        self.A = np.zeros((terms.__len__(), documents.__len__()), dtype='int8')\n",
        "\n",
        "        self._document_index_in_A = {doc:ix for ix,doc in enumerate(documents)}\n",
        "        self._term_index_in_A = {term:ix for ix,term in enumerate(terms)}\n",
        "        \n",
        "        for term in terms:\n",
        "            counter = self.index[term]\n",
        "            term_ix = self._term_index_in_A[term]\n",
        "            doc_ids = list(self.index[term].keys())\n",
        "            doc_vals = [counter[x] for x in doc_ids]\n",
        "            doc_ixs = [self._document_index_in_A[x] for x in doc_ids]\n",
        "            for ix,doc_id in enumerate(doc_ixs):\n",
        "                self.A[term_ix][doc_id] = doc_vals[ix]\n",
        "        print('Term-Document frequency matrix is ready.')\n",
        "        print('Proceeding to do SVD on the matrix.')\n",
        "        \n",
        "        self._svd_A()\n",
        "        self._get_k_for_svd()\n",
        "        \n",
        "        self.doc_rep = self.V[:self.k,:]\n",
        "        self.term_rep = self.U[:,:self.k]\n",
        "\n",
        "        print('Index Rebuilt. Setting __update_index to False. Queries can now be performed.')\n",
        "        self.__update_index = False\n",
        "        \n",
        "    def _calc_query_doc_affinity_score(self, query_vector):\n",
        "        '''\n",
        "        calculates the query - document affinity score\n",
        "        '''\n",
        "        try:\n",
        "            one_by_query_vector_norm_ = (1/norm(query_vector))\n",
        "        except ZeroDivisionError:\n",
        "            one_by_query_vector_norm_ = (1/1e-4)\n",
        "        affinity_scores = (np.dot(query_vector,self.doc_rep) / norm(self.doc_rep, axis=0)) * one_by_query_vector_norm_\n",
        "        return affinity_scores\n",
        "    \n",
        "    def query(self, query_string, top=5):\n",
        "        \n",
        "        if(self.__update_index == True):\n",
        "            print('Index is not updated. Use rebuild_index()')\n",
        "            return False\n",
        "        \n",
        "        query_vector = []\n",
        "        for token in [t.lower() for t in self.tokenizer(query_string) if t.isalpha()]:\n",
        "            if token in self.stopwords:\n",
        "                continue;\n",
        "            if self.stemmer:\n",
        "                token = self.stemmer.stem(token)\n",
        "            try:\n",
        "                query_vector.append(self.term_rep[self._term_index_in_A[token], :])\n",
        "            except KeyError:\n",
        "                query_vector.append(np.array([0.0] * self.k))\n",
        "        \n",
        "        query_vector_mean = np.array(query_vector).mean(axis=0)\n",
        "        affinity_scores = self._calc_query_doc_affinity_score(query_vector_mean)\n",
        "        \n",
        "        res_doc_index = (-affinity_scores).argsort()[:top]\n",
        "        results = []\n",
        "        for index in res_doc_index:\n",
        "            res_doc_id = list(self._document_index_in_A.keys())[index]\n",
        "            results.append(self.documents[res_doc_id])\n",
        "            \n",
        "        return results"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWMqxG8UttxX"
      },
      "source": [
        "lsi = LSI()\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    lsi.add_doc(row['extractive_summarized_3_sent'], index)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-_gchOvtusI",
        "outputId": "a3434027-24af-4304-b7d7-7794250cd3d8"
      },
      "source": [
        "# runs forever, not sure if viable\n",
        "lsi.rebuild_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Term-Document frequency matrix is ready.\n",
            "Proceeding to do SVD on the matrix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU_cJX4u-SVb"
      },
      "source": [
        "## Method 5: Sentence Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "tWJ81I7D-Tt0",
        "outputId": "f37ab129-140b-4a19-fb9d-6c6ece1e0d90"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "tagged_data = [TaggedDocument(d.split(), [idx]) for idx, d in df.iterrows()]\n",
        "tagged_data"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-98405ab3ae23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtagged_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-98405ab3ae23>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtagged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtagged_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5140\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5141\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQiLLf_v-TvX"
      },
      "source": [
        "def get_sentence_embedding(text):\n",
        "  tokenized = text.split()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPyCvVI5-Tyu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "estheDCt-T0X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-95u5UI-T2G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y_S4Nx9-T30"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKrIlELj-T5l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFM70qOc-T7W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnxmltOy-T9G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbKpgqhj-T-6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}