{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/gigi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/gigi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from pprint import pprint\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import os\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import gensim\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>cleaned_lowercase_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3243</th>\n",
       "      <td>the prime minister has the right to appoint a maximum of three such ministers, as the limit of ministers in one government is fifteen. it is also known as the cabinet. the cabinet carries out the country's domestic and foreign policy, shaped by parliament; it directs and co-ordinates the work of government institutions and bears full responsibility for everything occurring within the authority of executive power. the government, headed by the prime minister, thus represents the political leadership of the country and makes decisions in the name of the whole executive power.</td>\n",
       "      <td>What is another name for the governing body of ministers?</td>\n",
       "      <td>prime minister right appoint maximum three ministers limit minister one government fifteen also known cabinet cabinet carry country s domestic foreign policy shaped parliament directs co ordinate work government institution bear full responsibility everything occurring within authority executive power government headed prime minister thus represents political leadership country make decision name whole executive power</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   context  \\\n",
       "3243  the prime minister has the right to appoint a maximum of three such ministers, as the limit of ministers in one government is fifteen. it is also known as the cabinet. the cabinet carries out the country's domestic and foreign policy, shaped by parliament; it directs and co-ordinates the work of government institutions and bears full responsibility for everything occurring within the authority of executive power. the government, headed by the prime minister, thus represents the political leadership of the country and makes decisions in the name of the whole executive power.   \n",
       "\n",
       "                                                       question  \\\n",
       "3243  What is another name for the governing body of ministers?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                    cleaned_lowercase_nostop  \n",
       "3243  prime minister right appoint maximum three ministers limit minister one government fifteen also known cabinet cabinet carry country s domestic foreign policy shaped parliament directs co ordinate work government institution bear full responsibility everything occurring within authority executive power government headed prime minister thus represents political leadership country make decision name whole executive power   "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1785, the assembly of the Congress of the C...</td>\n",
       "      <td>In what year did New York become the United St...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In 1785, the assembly of the Congress of the C...</td>\n",
       "      <td>Who was the United States' first President?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In 1785, the assembly of the Congress of the C...</td>\n",
       "      <td>In what building did the Supreme Court of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1785, the assembly of the Congress of the C...</td>\n",
       "      <td>On what street did the writing of the Bill of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In 1785, the assembly of the Congress of the C...</td>\n",
       "      <td>What was the second largest city in the United...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  In 1785, the assembly of the Congress of the C...   \n",
       "1  In 1785, the assembly of the Congress of the C...   \n",
       "2  In 1785, the assembly of the Congress of the C...   \n",
       "3  In 1785, the assembly of the Congress of the C...   \n",
       "4  In 1785, the assembly of the Congress of the C...   \n",
       "\n",
       "                                            question  \n",
       "0  In what year did New York become the United St...  \n",
       "1        Who was the United States' first President?  \n",
       "2  In what building did the Supreme Court of the ...  \n",
       "3  On what street did the writing of the Bill of ...  \n",
       "4  What was the second largest city in the United...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs = pd.read_csv('legal_squad_data.csv', encoding='utf-8').loc[:, ['context', 'question']]\n",
    "question_pairs = question_pairs.drop_duplicates(subset=['context', 'question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def clean_lowercase_nostop(text):\n",
    "    text = ' '.join([wordnet_lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "#     text = ' '.join([ps.stem(word) for word in text.split() if word not in stop_words])\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\s{2,}', \" \", text)\n",
    "    text = unidecode(text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64186bf22364e469bc7e3feb0fae6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4855.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question_pairs['context'] = question_pairs['context'].apply(lambda x: str(x).lower())\n",
    "question_pairs['cleaned_lowercase_nostop'] = question_pairs['context'].progress_apply(clean_lowercase_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' new labour first termed alternative branding labour party dating conference slogan first used labour party 1994 later seen draft manifesto published party 1996 called new labour new life britain continuation trend begun leadership neil kinnock new labour name official status remains common use distinguish modernisers holding traditional positions normally referred old labour ',\n",
       " ' note 5 north carolina constitution 1776 disestablished anglican church 1835 nc constitution allowed protestant hold public office 1835 1876 allowed christian including catholics hold public office article vi section 8 current nc constitution forbids atheist holding public office clause held united state supreme court unenforceable 1961 case torcaso v watkins court ruled unanimously clause constituted religious test incompatible first fourteenth amendment protections ',\n",
       " ' religious test clause interpreted cover elected official appointed ones career civil servant well political appointees religious belief lack therefore permissible test qualification regard federal employee since ratification constitution seven states however language included bill rights declaration rights body constitution require state office holders particular religious beliefs though successfully challenged court state texas massachusetts maryland north carolina pennsylvania south carolina tennessee ',\n",
       " ' we used technique laying program general debate said got amendment phase would offer program substitute johnson proposal lost committee whole would usually offer motion recommit get vote that lost motion recommit republican member choice could vote johnson program say best come better alternative could vote make argument usually lost 140 435 expect win many ',\n",
       " ' yo yo opinion poll continued 1992 though november 1990 labour lead poll rarely sufficient majority major resisted kinnock s call general election throughout 1991 kinnock campaigned theme it s time change urging voter elect new government decade unbroken conservative rule however conservative undergone dramatic change change leader thatcher major least term style substance outset clearly well received change labour s 14 point lead november 1990 poll polls replaced 8 tory lead month later ',\n",
       " '1 april 2012 by election nld 43 45 available seats previously illegal organisation nld never burmese election time 2012 by election also first time international representative allowed monitor voting process myanmar following announcement by elections freedom house organisation raised concern reports fraud harassment lead elections including march 23 deportation somsri hananuntasuk executive director asian network free election anfrel regional network civil society organisation promoting democratization however uncertainty exist political prisoner released clash burmese troop local insurgent group continue ',\n",
       " '11 october 1962 first session second vatican council held vatican gave gaudet mater ecclesia speech served opening address council day basically electing member several council commission would work issue presented council night following conclusion first session people saint peter s square chanted yelled sole objective getting john xxiii appear window address them ',\n",
       " '12 july 2007 european court human right dismissing appeal nikola jorgic conviction genocide german court jorgic v germany noted german court wider interpretation genocide since rejected international court considering similar cases echr also noted 21st century amongst scholars majority taken view ethnic cleansing way carried serb force bosnia herzegovina order expel muslim croat homes constitute genocide however also considerable number scholar suggested act amount genocide icty found momcilo krajisnik case actus reu genocide met prijedor with regard charge genocide chamber found spite evidence act perpetrated municipality constituted actus reus genocide ',\n",
       " '12 year commissioner afl david baker retired unexpectedly july 25 2008 two day arenabowl xxii deputy commissioner ed policy named interim commissioner baker s replacement found baker explained when took commissioner thought would one year turned 12 time ',\n",
       " '1454 king casimir iv granted nieszawa statute polish statuty cerkwicko nieszawskie clarifying legal basis voivodship sejmiks local parliaments king could promulgate new laws raise taxes call levee en masse pospolite ruszenie consent sejmiks nobility protected judicial abuses nieszawa statute also curbed power magnates sejm national parliament received right elect many officials including judges voivods castellans privilege demanded szlachta compensation participation thirteen years war ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = question_pairs['cleaned_lowercase_nostop'].tolist()\n",
    "corpus = list(np.unique(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word, topn=5):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "      w for w in word.vocab \n",
    "      if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
    "    ]\n",
    "\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence (or document) objects  have vectors, derived from the averages of individual token vectors. This makes it possible to compare similarities between whole documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('The quick brown fox jumped over the lazy dogs.')\n",
    "len(doc.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import scipy.spatial\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 34s, sys: 12.6 s, total: 4min 47s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_embeddings = embedder.encode(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Candidate Genration using Faiss vector similarity search library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faiss is a library developed by Facebook AI Research. It is for effecient similarity search and clustering of dense vectors.\n",
    "\n",
    "**References:**\n",
    "\n",
    "1. [Tutorial](https://github.com/facebookresearch/faiss/wiki/Getting-started)\n",
    "2. [facebookresearch/faiss](https://github.com/facebookresearch/faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "1080\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "d = 768\n",
    "index = faiss.IndexFlatL2(d)\n",
    "print(index.is_trained)\n",
    "index.add(np.stack(corpus_embeddings, axis=0))\n",
    "print(index.is_trained)\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = ['What is the step by step guide to invest in share market in india?', 'How can Internet speed be increased by hacking through DNS?']\n",
    "orig_query = \"What is another name for the governing body of ministers?\"\n",
    "query = orig_query.lower()\n",
    "query = clean_lowercase_nostop(query)\n",
    "queries = [query]\n",
    "query_embeddings = embedder.encode(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 600   17  480 1030  497]]\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "D, I = index.search(np.stack(query_embeddings, axis=0), k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal Query: What is another name for the governing body of ministers?\n",
      "\n",
      "======================\n",
      "\n",
      "Query: another name governing body ministers \n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "council minister - presidency prime minister or president portugal latter s request minister may also include one deputy prime ministers - act cabinet government required define broad outline policy programme present assembly mandatory period debate failure assembly reject government programme absolute majority deputy confirms cabinet office  (Distance: 193.0990)\n",
      "\n",
      "signature parties representative follow end text treaty later reprinted collection treaty currently effect editor often append date respective party ratified treaty came effect party  (Distance: 204.3268)\n",
      "\n",
      "well head government prime minister may role titles--the prime minister united kingdom example also first lord treasury minister civil service prime minister may take ministerial posts--for example second world war winston churchill also minister defence although ministry defence current cabinet israel benjamin netanyahu also serf minister communications foreign affairs regional cooperation economy interior (Distance: 209.3913)\n",
      "\n",
      "prime minister senior minister cabinet executive branch government often parliamentary semi presidential system many systems prime minister selects may dismiss member cabinet allocates post member within government systems prime minister presiding member chairman cabinet minority systems notably semi presidential system government prime minister official appointed manage civil service execute directive head state  (Distance: 210.9702)\n",
      "\n",
      "preamble come numbered articles contain substance parties actual agreement article heading usually encompasses paragraph long treaty may group article chapter headings  (Distance: 212.6555)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Orignal Query:\", orig_query)\n",
    "for query, query_embedding in zip(queries, query_embeddings):\n",
    "    distances, indices = index.search(np.asarray(query_embedding).reshape(1,768),k)\n",
    "    print(\"\\n======================\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "    for idx in range(0,5):\n",
    "        print(corpus[indices[0,idx]], \"(Distance: %.4f)\" % distances[0,idx])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### did not use the code from here onwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Reranking using Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/bi_lstm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Reference:** https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "toko_tokenizer = ToktokTokenizer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_text(text):\n",
    "        puncts = ['/', ',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    "         '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    "         '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    "         '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    "         '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "        def clean_text(text):\n",
    "            text = str(text)\n",
    "            text = text.replace('\\n', '')\n",
    "            text = text.replace('\\r', '')\n",
    "            for punct in puncts:\n",
    "                if punct in text:\n",
    "                    text = text.replace(punct, '')\n",
    "            return text.lower()\n",
    "\n",
    "        def clean_numbers(text):\n",
    "            if bool(re.search(r'\\d', text)):\n",
    "                text = re.sub('[0-9]{5,}', '#####', text)\n",
    "                text = re.sub('[0-9]{4}', '####', text)\n",
    "                text = re.sub('[0-9]{3}', '###', text)\n",
    "                text = re.sub('[0-9]{2}', '##', text)\n",
    "            return text\n",
    "\n",
    "        contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "        def _get_contractions(contraction_dict):\n",
    "            contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "            return contraction_dict, contraction_re\n",
    "\n",
    "        contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "        def replace_contractions(text):\n",
    "            def replace(match):\n",
    "                return contractions[match.group(0)]\n",
    "            return contractions_re.sub(replace, text)\n",
    "\n",
    "        stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "        def remove_stopwords(text, is_lower_case=True):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            if is_lower_case:\n",
    "                filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "            else:\n",
    "                filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "            filtered_text = ' '.join(filtered_tokens)    \n",
    "            return filtered_text\n",
    "\n",
    "        def lemmatizer(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "        def trim_text(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            return ' '.join(tokens)\n",
    "        \n",
    "        def remove_non_english(text):\n",
    "            tokens = toko_tokenizer.tokenize(text)\n",
    "            tokens = [token.strip() for token in tokens]\n",
    "            tokens = [token for token in tokens if d.check(token)]\n",
    "            eng_text = ' '.join(tokens)\n",
    "            return eng_text\n",
    "\n",
    "        text_norm = clean_text(text)\n",
    "        text_norm = clean_numbers(text_norm)\n",
    "        text_norm = replace_contractions(text_norm)\n",
    "#         text_norm = remove_stopwords(text_norm)\n",
    "#         text_norm = remove_non_english(text_norm)\n",
    "        text_norm = lemmatizer(text_norm)\n",
    "        text_norm = trim_text(text_norm)\n",
    "        return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404351, 6)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./../../Embeddings/glove.twitter.27B/glove.twitter.27B.200d.txt\"\n",
    "def get_word2vec(file_path):\n",
    "    file = open(embedding_path, \"r\")\n",
    "    if (file):\n",
    "        word2vec = dict()\n",
    "        split = file.read().splitlines()\n",
    "        for line in split:\n",
    "            key = line.split(' ',1)[0]\n",
    "            value = np.array([float(val) for val in line.split(' ')[1:]])\n",
    "            word2vec[key] = value\n",
    "        return (word2vec)\n",
    "    else:\n",
    "        print(\"invalid fiel path\")\n",
    "w2v = get_word2vec(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_text = pd.concat([question_pairs['question1'], question_pairs['question2']]).reset_index(drop=True)\n",
    "total_text = total_text.apply(lambda x: str(x))\n",
    "total_text = total_text.apply(lambda x: normalize_text(x))\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(total_text)\n",
    "question_1_sequenced = tokenizer.texts_to_sequences(question_pairs['question1'].apply(lambda x: normalize_text(x)))\n",
    "question_2_sequenced = tokenizer.texts_to_sequences(question_pairs['question2'].apply(lambda x: normalize_text(x)))\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92423"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "question_1_padded = pad_sequences(question_1_sequenced, maxlen=maxlen)\n",
    "question_2_padded = pad_sequences(question_2_sequenced, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = question_pairs['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros\n",
    "embedding_matrix = zeros((vocab_size, 768))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "max_len = 100\n",
    "\n",
    "inp1 = Input(shape=(100,))\n",
    "inp2 = Input(shape=(100,))\n",
    "\n",
    "x1 = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=max_len)(inp1)\n",
    "x2 = Embedding(vocab_size, 200, weights=[embedding_matrix], input_length=max_len)(inp2)\n",
    "\n",
    "x3 = Bidirectional(LSTM(32, return_sequences = True))(x1)\n",
    "x4 = Bidirectional(LSTM(32, return_sequences = True))(x2)\n",
    "\n",
    "x5 = GlobalMaxPool1D()(x3)\n",
    "x6 = GlobalMaxPool1D()(x4)\n",
    "\n",
    "x7 =  dot([x5, x6], axes=1)\n",
    "\n",
    "x8 = Dense(40, activation='relu')(x7)\n",
    "x9 = Dropout(0.05)(x8)\n",
    "x10 = Dense(10, activation='relu')(x9)\n",
    "output = Dense(1, activation=\"sigmoid\")(x10)\n",
    "\n",
    "model = Model(inputs=[inp1, inp2], outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 256\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323480 samples, validate on 80871 samples\n",
      "Epoch 1/4\n",
      "323480/323480 [==============================] - 1096s 3ms/step - loss: 0.5212 - acc: 0.7372 - val_loss: 0.4596 - val_acc: 0.7802\n",
      "Epoch 2/4\n",
      "323480/323480 [==============================] - 1101s 3ms/step - loss: 0.4169 - acc: 0.8038 - val_loss: 0.4300 - val_acc: 0.7978\n",
      "Epoch 3/4\n",
      "323480/323480 [==============================] - 1135s 4ms/step - loss: 0.3536 - acc: 0.8400 - val_loss: 0.4340 - val_acc: 0.7976\n",
      "Epoch 4/4\n",
      "323480/323480 [==============================] - 1077s 3ms/step - loss: 0.2963 - acc: 0.8701 - val_loss: 0.4411 - val_acc: 0.8035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd352da0>"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([question_1_padded, question_2_padded], y, batch_size=batch_size, epochs=epochs, validation_split=0.2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Combining candidate generation and reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How will Indian GDP be affected from banning 500 and 1000 rupees notes?'"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_copy = [query]*len(relevant_docs)\n",
    "question_1_sequenced_final = tokenizer.texts_to_sequences(query_copy)\n",
    "question_2_sequenced_final = tokenizer.texts_to_sequences(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "question_1_padded_final = pad_sequences(question_1_sequenced_final, maxlen=maxlen)\n",
    "question_2_padded_final = pad_sequences(question_2_sequenced_final, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = model.predict([question_1_padded_final, question_2_padded_final])\n",
    "preds_test = np.array([x[0] for x in preds_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What do you think about banning 500 and 1000 rupee notes in India?',\n",
       " 'What will be the implications of banning 500 and 1000 rupees currency notes on Indian economy?',\n",
       " 'What will be the consequences of 500 and 1000 rupee notes banning?',\n",
       " 'What will be the effects after banning on 500 and 1000 rupee notes?',\n",
       " 'What will be the impact on real estate by banning 500 and 1000 rupee notes from India?',\n",
       " 'How is banning 500 and 1000 INR going to help Indian economy?',\n",
       " 'What are your views on India banning 500 and 1000 notes? In what way it will affect Indian economy?',\n",
       " 'How is discontinuing 500 and 1000 rupee note going to put a hold on black money in India?',\n",
       " 'What will be the result of banning 500 and 1000 rupees note in India?',\n",
       " 'What are the economic implications of banning 500 and 1000 rupee notes?']"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[relevant_docs[x] for x in preds_test.argsort()[::-1]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
